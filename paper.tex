\documentclass{article}
\usepackage{iclr2026_conference,times}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{placeins}

\title{When Do LLM Forecasters Beat Prediction Markets? \\ Evidence from 15,000 Probabilistic Forecasts on CFTC-Regulated Markets}

\author{Anonymous Authors}

\begin{document}

\maketitle

\begin{abstract}
We present HADC-Bench, evaluating when LLMs beat prediction markets. Across 10 models and 150 CFTC-regulated markets, we generate 15,000 predictions at five temporal checkpoints. Key findings: \textbf{(1) Temporal degradation is universal}—models beat markets early (Claude +0.167 BSS at Open+1) but fail late (all BSS $<$ -0.7 at Close-1) as continuous market aggregation overwhelms periodic updates. \textbf{(2) Web search is critical}—improves BSS by 0.1-0.6 points. \textbf{(3) Contrarian edge}—Claude achieves 69.2\% accuracy when strongly disagreeing at mid-lifecycle. \textbf{(4) Domain specialists}—Claude dominates sports (+0.294) and macro (+0.163); all fail politics and finance. These findings motivate temporal-domain task routing.
\end{abstract}

\section{Introduction}

\subsection{Motivation}

When should we trust an LLM's prediction over a prediction market? Existing benchmarks use static question sets, making temporal dynamics invisible—yet markets aggregate information continuously, with early prices reflecting sparse information and late prices incorporating near-complete information. We address this gap: \textbf{Under what temporal conditions do LLM forecasters beat prediction markets?}

\subsection{Our Approach and Key Findings}

We evaluate 10 frontier models as \textbf{agentic forecasters} on 150 CFTC-regulated binary markets from Kalshi. All markets feature verified outcomes, real capital at risk (\$5k+ volume), and post-training resolution (events after model training cutoffs). We sample at 5 lifecycle checkpoints—Open+1 (1\%), 25\%, 50\%, 75\%, Close-1 (99\%)—generating 15,000 predictions.

Our evaluation reveals: \textbf{(1)} Four models beat markets early (Claude +0.167, Kimi-k2.5 +0.118, GPT +0.085, Kimi-k2 +0.011 BSS at Open+1), but all fail late (BSS $< -0.7$ at Close-1)—continuous market aggregation overwhelms periodic model updates. \textbf{(2)} Web search is critical: without it, BSS drops 0.1-0.6 points. \textbf{(3)} Domain specialists emerge: Claude dominates sports (+0.294) and macro (+0.163); all models fail politics and finance. \textbf{(4)} When Claude strongly disagrees with markets at 25\%, it achieves 69.2\% accuracy. These results motivate temporal-domain task routing.

\section{Method}

\subsection{Dataset: CFTC-Regulated Binary Markets}

We collect 150 binary markets from Kalshi, a CFTC-regulated exchange. Each market resolves to YES/NO based on objective criteria (election results, economic releases, sports scores). We enforce \$5,000+ trading volume per market and require post-October 2025 resolution to ensure temporal separation from model training cutoffs. The dataset spans five categories (30 markets each): Politics, Sports, Macro-Economics, Science/Tech, and Financial markets. Table~\ref{tab:dataset_characteristics} provides statistics.

\begin{table}[t]
\centering
\caption{HADC-Bench dataset characteristics.}
\label{tab:dataset_characteristics}
\small
\begin{tabular}{lr}
\toprule
\textbf{Characteristic} & \textbf{Value} \\
\midrule
Markets / Checkpoints / Predictions & 150 / 5 / 750 \\
Categories & 5 (30 markets each) \\
Timeline & Jan 2025 -- Jan 2026 \\
\midrule
Trading volume & \$5.6k -- \$41.3M (med. \$35k) \\
Duration (days) & 2 -- 337 (med. 89) \\
Outcomes & 73\% NO, 27\% YES \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Temporal Sampling Protocol}

For each market, we define lifecycle progress $p(t) = \frac{t - t_{\text{open}}}{t_{\text{close}} - t_{\text{open}}}$ and sample at 5 checkpoints: Open+1 ($p{=}0.01$), 25\%, 50\%, 75\%, and Close-1 ($p{=}0.99$). This percentage-based approach ensures comparable positions across heterogeneous durations. For each sample, we record sample date, market price, and ground truth outcome, yielding 750 samples per condition (1,500 per model, 15,000 total).

\subsection{Agentic Forecasting Architecture}

We implement agentic forecasters using LangGraph with a 4-node workflow: research (initialize context), agent (decide to search or predict), tools (execute searches), and forecast (extract structured output). The agent can invoke web search multiple times before forecasting.

All models receive identical system prompts instructing calibrated probabilistic forecasting. When web search is enabled, agents access a search tool backed by SerpAPI with date filtering (\texttt{cd\_max: sample\_date}) to prevent future data leakage. Models use temperatures 0.7-1.0 depending on provider constraints. Market prices are hidden during prediction to prevent anchoring—we compare model predictions to market prices retrospectively.

\subsection{Models Evaluated}

We evaluate 10 frontier LLMs: Claude Opus 4.5, GPT-5.2-xhigh, Gemini 3 Pro, Grok 4.1-fast, Kimi-k2, Kimi-k2.5, DeepSeek v3.2, Intellect-3, Trinity Large, and Qwen3-235B. All models were released October 2025 or later and represent decoder-only transformers ranging from 70B to 600B+ parameters. Each model generates 750 predictions per tool condition (with/without web search), totaling 15,000 predictions.

\subsection{Evaluation Metrics}

\textbf{Brier Score (BS):} $\text{BS} = \frac{1}{N}\sum_{i=1}^{N}(p_i - y_i)^2$, where $p_i$ is predicted probability and $y_i \in \{0, 1\}$.

\textbf{Brier Skill Score (BSS):} $\text{BSS} = 1 - \frac{\text{BS}_{\text{model}}}{\text{BS}_{\text{market}}}$, where $\text{BS}_{\text{market}}$ uses market price at prediction time. Positive BSS indicates beating markets.

We also report Expected Calibration Error (ECE) and accuracy for reference.

\section{Results}

Tables~\ref{tab:performance_agentic} and~\ref{tab:performance_baseline} present BSS for all models with and without web search.

% Table: Model Performance (WITH WEB SEARCH)
\begin{table*}[t]
\centering
\caption{Brier Skill Score (BSS) across 5 temporal checkpoints \textbf{with web search}. Positive BSS indicates beating market baseline.}
\label{tab:performance_agentic}
\small
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Model} & \textbf{Open+1} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{Close-1} & \textbf{Average} \\
\midrule
Claude Opus 4.5 & \textbf{+0.167} & \textbf{+0.059} & -0.136 & -0.165 & -0.829 & -0.069 \\
GPT-5.2-xhigh & \textbf{+0.085} & -0.095 & -0.344 & -0.595 & -0.960 & -0.252 \\
Gemini 3 Pro & -0.084 & -0.250 & -0.373 & -0.510 & -0.734 & -0.312 \\
Grok 4.1-fast & -0.003 & -0.071 & -0.314 & -0.621 & -0.909 & -0.267 \\
Kimi-k2 & \textbf{+0.011} & -0.113 & -0.601 & -0.912 & -0.823 & -0.367 \\
Kimi-k2.5 & \textbf{+0.118} & -0.022 & -0.311 & -0.579 & -0.942 & -0.214 \\
DeepSeek v3.2 & -0.064 & -0.424 & -0.690 & -0.870 & -1.158 & -0.507 \\
Intellect-3 & -0.057 & -0.433 & -0.641 & -0.698 & -1.736 & -0.527 \\
Trinity Large & -0.128 & -0.275 & -0.581 & -0.684 & -1.716 & -0.496 \\
Qwen3-235B & -0.132 & -0.401 & -0.955 & -1.018 & -1.334 & -0.615 \\
\bottomrule
\end{tabular}
\end{table*}

% Table: Model Performance (NO WEB SEARCH)
\begin{table*}[t]
\centering
\caption{Brier Skill Score (BSS) for all 10 models across 5 temporal checkpoints \textbf{without web search} (baseline, parametric knowledge only). Performance is substantially worse without external information access.}
\label{tab:performance_baseline}
\small
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Model} & \textbf{Open+1} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{Close-1} & \textbf{Average} \\
\midrule
Claude Opus 4.5 & -0.023 & -0.319 & -0.676 & -0.981 & -2.212 & -0.588 \\
GPT-5.2-xhigh & \textbf{+0.125} & -0.202 & -0.474 & -0.682 & -1.853 & -0.392 \\
Gemini 3 Pro & -0.004 & -0.469 & -0.618 & -1.034 & -2.427 & -0.636 \\
Grok 4.1-fast & -0.217 & -0.528 & -1.036 & -1.300 & -2.619 & -0.859 \\
Kimi-k2 & \textbf{+0.036} & -0.347 & -0.466 & -0.766 & -2.211 & -0.501 \\
Kimi-k2.5 & \textbf{+0.008} & -0.202 & -0.483 & -0.774 & -1.719 & -0.432 \\
DeepSeek v3.2 & -0.116 & -0.514 & -0.787 & -1.064 & -2.202 & -0.697 \\
Intellect-3 & -0.192 & -0.385 & -0.748 & -1.361 & -2.465 & -0.755 \\
Trinity Large & -0.129 & -0.423 & -0.970 & -0.989 & -2.532 & -0.735 \\
Qwen3-235B & -0.168 & -0.681 & -1.018 & -1.119 & -2.750 & -0.861 \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Web Search is Critical}

Web search improves average BSS by 0.1-0.6 points across all models: Claude gains +0.519 (-0.588 to -0.069), Grok +0.592, Gemini +0.324, GPT +0.140. Without search, Claude's Close-1 BSS drops to -2.212—parametric knowledge alone is insufficient.

\subsection{Temporal Degradation is Universal}

Four models beat markets at Open+1 (Claude +0.167, Kimi-k2.5 +0.118, GPT +0.085, Kimi-k2 +0.011), but all fail by Close-1 (BSS $< -0.7$). The pattern is monotonic: degradation ranges from -0.65 (Gemini) to -1.68 (Intellect-3). Average BSS stratifies into tiers: Tier 1 (Claude -0.069, Kimi-k2.5 -0.214, GPT -0.252, Grok -0.267), Tier 2 (Gemini -0.312, Kimi-k2 -0.367), Tier 3 (DeepSeek -0.507, Intellect -0.527, Qwen3 -0.615). This demonstrates LLMs with periodic web search cannot track continuous market information aggregation.

\subsection{Search Behavior and Confidence}

Table~\ref{tab:search_counts} shows search counts per prediction. Claude searches aggressively (12-13), Kimi-k2.5 minimally (3-4). Search quantity does not correlate with performance—search quality matters more.

\begin{table*}[t]
\centering
\caption{Average web searches per prediction across temporal checkpoints.}
\label{tab:search_counts}
\small
\begin{tabular}{lrrrrr}
\toprule
\textbf{Model} & \textbf{Open+1} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{Close-1} \\
\midrule
Claude Opus 4.5 & 12.7 & 13.4 & 12.5 & 13.0 & 10.2 \\
GPT-5.2-xhigh & 6.8 & 8.0 & 9.9 & 7.1 & 9.4 \\
Gemini 3 Pro & 6.3 & 6.6 & 6.5 & 6.6 & 5.4 \\
Grok 4.1-fast & 10.4 & 10.4 & 10.1 & 10.0 & 8.4 \\
Kimi-k2 & 5.0 & 5.1 & 5.0 & 5.0 & 4.5 \\
Kimi-k2.5 & 3.5 & 3.6 & 3.5 & 3.8 & 3.2 \\
DeepSeek v3.2 & 6.0 & 5.8 & 6.1 & 5.9 & 6.2 \\
Intellect-3 & 1.9 & 1.9 & 1.8 & 1.7 & 1.6 \\
Trinity Large & 7.5 & 5.7 & 6.0 & 5.8 & 5.1 \\
Qwen3-235B & 2.5 & 1.6 & 2.4 & 1.3 & 1.2 \\
\bottomrule
\end{tabular}
\end{table*}

Confidence patterns reveal miscalibration: DeepSeek and Qwen3 show highest confidence (45-47\%) but worst BSS, while Claude's moderate confidence (38-42\%) yields best BSS. Overconfident incorrect predictions accumulate large Brier penalties.

\subsection{Belief Updates and Contrarian Accuracy}

Table~\ref{tab:belief_updates} shows belief dynamics: six models converge toward markets (Grok -0.052, Gemini -0.050), while four diverge. High-search models converge; low-search models diverge, suggesting extensive search corrects biases.

\begin{table}[t]
\centering
\caption{Belief update dynamics. $\Delta$ Change: negative = converging toward market.}
\label{tab:belief_updates}
\small
\begin{tabular}{lrr}
\toprule
\textbf{Model} & \textbf{$\Delta$ Change} & \textbf{Avg Searches} \\
\midrule
Claude Opus 4.5 & -0.006 & 12.4 \\
GPT-5.2-xhigh & +0.009 & 8.3 \\
Gemini 3 Pro & -0.050 & 6.3 \\
Grok 4.1-fast & -0.052 & 9.9 \\
Kimi-k2 & -0.041 & 4.9 \\
Kimi-k2.5 & +0.035 & 3.5 \\
DeepSeek v3.2 & -0.008 & 6.0 \\
Intellect-3 & +0.028 & 1.8 \\
Trinity Large & -0.024 & 6.0 \\
Qwen3-235B & +0.010 & 1.8 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:contrarian_accuracy} shows accuracy when models strongly disagree ($>$30\% divergence). Claude achieves 69.2\% at 25\% and 64.4\% at 50\%—finding information markets miss. By Close-1, contrarian predictions become unreliable (35-50\%).

\begin{table*}[t]
\centering
\caption{Contrarian accuracy (divergence $>$ 30\%). Bold indicates above-chance performance.}
\label{tab:contrarian_accuracy}
\small
\begin{tabular}{lrrrrr}
\toprule
\textbf{Model} & \textbf{Open+1} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{Close-1} \\
\midrule
Claude Opus 4.5 & 54.8\% (\textit{n=42}) & \textbf{69.2\%} (\textit{n=39}) & \textbf{64.4\%} (\textit{n=45}) & 59.6\% (\textit{n=47}) & 47.7\% (\textit{n=44}) \\
GPT-5.2-xhigh & 56.4\% (\textit{n=39}) & 48.5\% (\textit{n=33}) & 47.4\% (\textit{n=38}) & 35.0\% (\textit{n=40}) & 46.3\% (\textit{n=41}) \\
Gemini 3 Pro & 52.5\% (\textit{n=59}) & 48.8\% (\textit{n=41}) & 47.9\% (\textit{n=48}) & 46.7\% (\textit{n=45}) & 48.6\% (\textit{n=37}) \\
Grok 4.1-fast & 56.1\% (\textit{n=41}) & 58.3\% (\textit{n=36}) & 41.2\% (\textit{n=34}) & 43.2\% (\textit{n=37}) & 40.0\% (\textit{n=30}) \\
Kimi-k2 & 57.4\% (\textit{n=47}) & 57.8\% (\textit{n=45}) & 36.4\% (\textit{n=44}) & 35.3\% (\textit{n=51}) & 50.0\% (\textit{n=40}) \\
Kimi-k2.5 & 52.6\% (\textit{n=38}) & 56.2\% (\textit{n=32}) & 40.0\% (\textit{n=45}) & 39.6\% (\textit{n=48}) & 48.0\% (\textit{n=50}) \\
DeepSeek v3.2 & 38.9\% (\textit{n=54}) & 35.3\% (\textit{n=51}) & 35.1\% (\textit{n=57}) & 35.3\% (\textit{n=51}) & 35.6\% (\textit{n=45}) \\
Intellect-3 & 48.2\% (\textit{n=56}) & 42.6\% (\textit{n=61}) & 33.3\% (\textit{n=51}) & 40.8\% (\textit{n=49}) & 32.7\% (\textit{n=49}) \\
Trinity Large & 49.2\% (\textit{n=59}) & 50.0\% (\textit{n=62}) & 32.7\% (\textit{n=52}) & 38.2\% (\textit{n=55}) & 25.5\% (\textit{n=47}) \\
Qwen3-235B & 49.0\% (\textit{n=49}) & 42.0\% (\textit{n=50}) & 27.1\% (\textit{n=59}) & 34.9\% (\textit{n=63}) & 38.8\% (\textit{n=49}) \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Domain Specialists Emerge}

Table~\ref{tab:category_performance} shows domain-specific performance. Claude dominates Sports (+0.294) and Macro (+0.163); GPT/Gemini show positive Macro BSS. All models fail Politics and Financial. This motivates domain routing: deploy Claude for sports/macro, defer to markets for politics/finance.

\begin{table*}[t]
\centering
\caption{BSS by category. Bold = beating markets.}
\label{tab:category_performance}
\small
\begin{tabular}{lrrrrr}
\toprule
\textbf{Model} & \textbf{Politics} & \textbf{Sports} & \textbf{Macro} & \textbf{Sci/Tech} & \textbf{Financial} \\
\midrule
Claude Opus 4.5 & -0.379 & \textbf{+0.294} & \textbf{+0.163} & -0.154 & -0.609 \\
GPT-5.2-xhigh & -0.484 & -0.088 & \textbf{+0.021} & -0.378 & -0.482 \\
Gemini 3 Pro & -0.598 & -0.227 & \textbf{+0.033} & -0.575 & -0.217 \\
Grok 4.1-fast & -0.427 & -0.197 & -0.182 & -0.190 & -0.426 \\
Kimi-k2 & -0.421 & -0.342 & -0.023 & -0.369 & -0.788 \\
Kimi-k2.5 & -0.596 & \textbf{+0.124} & -0.066 & -0.213 & -0.625 \\
DeepSeek v3.2 & -0.703 & -0.110 & -0.333 & -0.596 & -1.182 \\
Intellect-3 & -0.706 & -0.255 & -0.275 & -0.569 & -1.131 \\
Trinity Large & -0.376 & -0.231 & -0.342 & -0.605 & -1.232 \\
Qwen3-235B & -1.020 & -0.046 & -0.601 & -0.719 & -1.158 \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Convergence Speed}

Table~\ref{tab:convergence_speed} shows how often models reach correct predictions before markets. Gemini/Grok converge fastest early (56.7\% at Open+1), but by Close-1 all models lag. Gemini/Grok converge faster but have worse BSS than Claude—directionally correct but poorly calibrated.

\begin{table*}[htbp]
\centering
\caption{Percentage where model error $<$ market error. Bold = $>$50\%.}
\label{tab:convergence_speed}
\small
\begin{tabular}{lrrrrr}
\toprule
\textbf{Model} & \textbf{Open+1} & \textbf{25\%} & \textbf{50\%} & \textbf{75\%} & \textbf{Close-1} \\
\midrule
Claude Opus 4.5 & 50.0\% & 49.3\% & 41.3\% & 44.0\% & 31.3\% \\
GPT-5.2-xhigh & 50.0\% & 43.3\% & 36.7\% & 34.7\% & 36.0\% \\
Gemini 3 Pro & \textbf{56.7\%} & 49.3\% & \textbf{50.7\%} & 48.7\% & 42.7\% \\
Grok 4.1-fast & \textbf{56.7\%} & \textbf{54.0\%} & 48.0\% & 39.3\% & 46.0\% \\
Kimi-k2 & 50.0\% & 47.3\% & 34.7\% & 37.3\% & 35.3\% \\
Kimi-k2.5 & 46.7\% & 47.3\% & 35.3\% & 34.7\% & 35.3\% \\
DeepSeek v3.2 & 44.0\% & 34.0\% & 30.7\% & 27.3\% & 31.3\% \\
Intellect-3 & 48.0\% & 34.7\% & 30.7\% & 32.7\% & 20.7\% \\
Trinity Large & 47.3\% & 44.0\% & 41.3\% & 38.7\% & 22.7\% \\
Qwen3-235B & 46.7\% & 40.0\% & 30.0\% & 32.7\% & 28.7\% \\
\bottomrule
\end{tabular}
\end{table*}

\FloatBarrier
\section{Discussion}

\subsection{Temporal Task Routing}

Deploy LLMs for early-stage forecasts (Open+1, 25\%) where positive BSS demonstrates value over sparse market information; defer to markets for late-stage forecasts where continuous trading has aggregated real-time information. The degradation is not calibration failure—it is information staleness: periodic web search cannot match continuous market updates.

\subsection{Limitations}

\textbf{Dataset:} 150 markets represent a sample; excluding $<$\$5k volume may miss tail markets. \textbf{Granularity:} 5 checkpoints are coarse; finer evaluation would reveal exact crossover points. \textbf{Models:} 10 models miss competitors (GPT-o3, Claude Sonnet 4) that may differ. \textbf{Tools:} Web search only; richer tools might extend LLM advantage.

\subsection{Future Work}

\textbf{RLMF post-training:} Fine-tune models on prediction tasks with BSS rewards to learn when to defer to markets. \textbf{Hybrid systems:} Combine LLM reasoning, human judgment, and market aggregation—Claude's 69.2\% contrarian accuracy suggests models find information markets miss. \textbf{Expanded benchmarking:} Scale to thousands of markets, continuous-valued predictions, and international coverage.

\end{document}